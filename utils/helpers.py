# -*- coding: utf-8 -*-
"""
è¾…åŠ©å·¥å…·æ¨¡å— / Helper Utilities Module
=================================

æœ¬æ¨¡å—æä¾›ç­–ç•¥ç³»ç»Ÿæ‰€éœ€çš„å„ç§è¾…åŠ©åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
This module provides various helper functions needed by the strategy system, including:

1. æŠ€æœ¯æŒ‡æ ‡è®¡ç®— / Technical indicator calculations
2. æ•°æ®éªŒè¯å’Œå¤„ç† / Data validation and processing
3. æ–‡ä»¶I/Oæ“ä½œ / File I/O operations
4. æ—¥æœŸæ—¶é—´å¤„ç† / Date and time processing
5. æ•°å­¦ç»Ÿè®¡å‡½æ•° / Mathematical and statistical functions

å…³è”æ–‡ä»¶ / Related Files:
- strategies/pump_short_strategy.py: ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡ / Uses technical indicators
- run_backtest.py: ä½¿ç”¨æ•°æ®å¤„ç†åŠŸèƒ½ / Uses data processing functions
- core/portfolio.py: ä½¿ç”¨ç»Ÿè®¡è®¡ç®— / Uses statistical calculations

ä¸»è¦åŠŸèƒ½ / Main Functions:
1. è®¡ç®—å„ç§æŠ€æœ¯æŒ‡æ ‡ï¼ˆRSI, MACD, SMAç­‰ï¼‰/ Calculate technical indicators
2. æ•°æ®æ¸…æ´—å’ŒéªŒè¯ / Data cleaning and validation
3. é…ç½®æ–‡ä»¶åŠ è½½å’Œä¿å­˜ / Configuration file loading and saving
4. æ—¥å¿—è®°å½•è¾…åŠ©åŠŸèƒ½ / Logging helper functions
"""

import pandas as pd
import numpy as np
import yaml
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
import logging
from pathlib import Path

class TechnicalIndicators:
    """
    æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç±» / Technical Indicators Class
    
    æä¾›å„ç§å¸¸ç”¨æŠ€æœ¯æŒ‡æ ‡çš„è®¡ç®—åŠŸèƒ½ã€‚
    Provides calculation functions for various common technical indicators.
    """
    
    @staticmethod
    def sma(data: pd.Series, period: int) -> pd.Series:
        """
        ç®€å•ç§»åŠ¨å¹³å‡çº¿ / Simple Moving Average
        
        Args:
            data (pd.Series): ä»·æ ¼æ•°æ® / Price data
            period (int): å‘¨æœŸ / Period
            
        Returns:
            pd.Series: SMAæ•°æ® / SMA data
        """
        return data.rolling(window=period).mean()
    
    @staticmethod
    def ema(data: pd.Series, period: int) -> pd.Series:
        """
        æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿ / Exponential Moving Average
        
        Args:
            data (pd.Series): ä»·æ ¼æ•°æ® / Price data
            period (int): å‘¨æœŸ / Period
            
        Returns:
            pd.Series: EMAæ•°æ® / EMA data
        """
        return data.ewm(span=period).mean()
    
    @staticmethod
    def rsi(data: pd.Series, period: int = 14) -> pd.Series:
        """
        ç›¸å¯¹å¼ºå¼±æŒ‡æ•° / Relative Strength Index
        
        Args:
            data (pd.Series): ä»·æ ¼æ•°æ® / Price data
            period (int): å‘¨æœŸï¼Œé»˜è®¤14 / Period, default 14
            
        Returns:
            pd.Series: RSIæ•°æ® / RSI data
        """
        delta = data.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi
    
    @staticmethod
    def bollinger_bands(data: pd.Series, period: int = 20, std_dev: float = 2.0) -> Dict[str, pd.Series]:
        """
        å¸ƒæ—å¸¦ / Bollinger Bands
        
        Args:
            data (pd.Series): ä»·æ ¼æ•°æ® / Price data
            period (int): å‘¨æœŸï¼Œé»˜è®¤20 / Period, default 20
            std_dev (float): æ ‡å‡†å·®å€æ•°ï¼Œé»˜è®¤2.0 / Standard deviation multiplier, default 2.0
            
        Returns:
            Dict[str, pd.Series]: åŒ…å«ä¸Šè½¨ã€ä¸­è½¨ã€ä¸‹è½¨çš„å­—å…¸ / Dictionary containing upper, middle, lower bands
        """
        sma = data.rolling(window=period).mean()
        std = data.rolling(window=period).std()
        
        upper_band = sma + (std * std_dev)
        lower_band = sma - (std * std_dev)
        
        return {
            'upper': upper_band,
            'middle': sma,
            'lower': lower_band
        }
    
    @staticmethod
    def macd(data: pd.Series, fast_period: int = 12, slow_period: int = 26, signal_period: int = 9) -> Dict[str, pd.Series]:
        """
        MACDæŒ‡æ ‡ / MACD Indicator
        
        Args:
            data (pd.Series): ä»·æ ¼æ•°æ® / Price data
            fast_period (int): å¿«çº¿å‘¨æœŸï¼Œé»˜è®¤12 / Fast period, default 12
            slow_period (int): æ…¢çº¿å‘¨æœŸï¼Œé»˜è®¤26 / Slow period, default 26
            signal_period (int): ä¿¡å·çº¿å‘¨æœŸï¼Œé»˜è®¤9 / Signal period, default 9
            
        Returns:
            Dict[str, pd.Series]: åŒ…å«MACDçº¿ã€ä¿¡å·çº¿ã€æŸ±çŠ¶å›¾çš„å­—å…¸ / Dictionary containing MACD line, signal line, histogram
        """
        ema_fast = data.ewm(span=fast_period).mean()
        ema_slow = data.ewm(span=slow_period).mean()
        
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal_period).mean()
        histogram = macd_line - signal_line
        
        return {
            'macd': macd_line,
            'signal': signal_line,
            'histogram': histogram
        }
    
    @staticmethod
    def stochastic(high: pd.Series, low: pd.Series, close: pd.Series, k_period: int = 14, d_period: int = 3) -> Dict[str, pd.Series]:
        """
        éšæœºæŒ¯è¡å™¨ / Stochastic Oscillator
        
        Args:
            high (pd.Series): æœ€é«˜ä»· / High prices
            low (pd.Series): æœ€ä½ä»· / Low prices
            close (pd.Series): æ”¶ç›˜ä»· / Close prices
            k_period (int): Kå‘¨æœŸï¼Œé»˜è®¤14 / K period, default 14
            d_period (int): Då‘¨æœŸï¼Œé»˜è®¤3 / D period, default 3
            
        Returns:
            Dict[str, pd.Series]: åŒ…å«%Kå’Œ%Dçº¿çš„å­—å…¸ / Dictionary containing %K and %D lines
        """
        lowest_low = low.rolling(window=k_period).min()
        highest_high = high.rolling(window=k_period).max()
        
        k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))
        d_percent = k_percent.rolling(window=d_period).mean()
        
        return {
            'k_percent': k_percent,
            'd_percent': d_percent
        }
    
    @staticmethod
    def volume_weighted_average_price(high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:
        """
        æˆäº¤é‡åŠ æƒå¹³å‡ä»· / Volume Weighted Average Price (VWAP)
        
        Args:
            high (pd.Series): æœ€é«˜ä»· / High prices
            low (pd.Series): æœ€ä½ä»· / Low prices
            close (pd.Series): æ”¶ç›˜ä»· / Close prices
            volume (pd.Series): æˆäº¤é‡ / Volume
            
        Returns:
            pd.Series: VWAPæ•°æ® / VWAP data
        """
        typical_price = (high + low + close) / 3
        vwap = (typical_price * volume).cumsum() / volume.cumsum()
        
        return vwap

class DataValidator:
    """
    æ•°æ®éªŒè¯å™¨ç±» / Data Validator Class
    
    æä¾›æ•°æ®è´¨é‡æ£€æŸ¥å’ŒéªŒè¯åŠŸèƒ½ã€‚
    Provides data quality checking and validation functions.
    """
    
    @staticmethod
    def validate_kline_data(df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """
        éªŒè¯Kçº¿æ•°æ®æ ¼å¼å’Œè´¨é‡ / Validate candlestick data format and quality
        
        Args:
            df (pd.DataFrame): Kçº¿æ•°æ® / Candlestick data
            
        Returns:
            Tuple[bool, List[str]]: (æ˜¯å¦é€šè¿‡éªŒè¯, é”™è¯¯ä¿¡æ¯åˆ—è¡¨) / (Whether passed validation, error messages list)
        """
        errors = []
        
        try:
            # æ£€æŸ¥å¿…éœ€åˆ— / Check required columns
            required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                errors.append(f"ç¼ºå°‘å¿…éœ€åˆ— / Missing required columns: {missing_columns}")
            
            # æ£€æŸ¥æ•°æ®ç±»å‹ / Check data types
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                if col in df.columns:
                    if not pd.api.types.is_numeric_dtype(df[col]):
                        errors.append(f"åˆ— {col} ä¸æ˜¯æ•°å€¼ç±»å‹ / Column {col} is not numeric")
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ / Check data integrity
            if not df.empty:
                # æ£€æŸ¥ä»·æ ¼é€»è¾‘ / Check price logic
                invalid_prices = df[df['high'] < df['low']]
                if not invalid_prices.empty:
                    errors.append(f"å‘ç° {len(invalid_prices)} æ¡æœ€é«˜ä»·ä½äºæœ€ä½ä»·çš„è®°å½• / Found {len(invalid_prices)} records where high < low")
                
                invalid_ohlc = df[(df['open'] > df['high']) | (df['open'] < df['low']) | 
                                (df['close'] > df['high']) | (df['close'] < df['low'])]
                if not invalid_ohlc.empty:
                    errors.append(f"å‘ç° {len(invalid_ohlc)} æ¡å¼€ç›˜ä»·æˆ–æ”¶ç›˜ä»·è¶…å‡ºæœ€é«˜æœ€ä½ä»·èŒƒå›´çš„è®°å½• / Found {len(invalid_ohlc)} records with open/close outside high/low range")
                
                # æ£€æŸ¥è´Ÿå€¼ / Check negative values
                negative_prices = df[(df['open'] <= 0) | (df['high'] <= 0) | (df['low'] <= 0) | (df['close'] <= 0)]
                if not negative_prices.empty:
                    errors.append(f"å‘ç° {len(negative_prices)} æ¡ä»·æ ¼ä¸ºè´Ÿæˆ–é›¶çš„è®°å½• / Found {len(negative_prices)} records with negative or zero prices")
                
                negative_volume = df[df['volume'] < 0]
                if not negative_volume.empty:
                    errors.append(f"å‘ç° {len(negative_volume)} æ¡æˆäº¤é‡ä¸ºè´Ÿçš„è®°å½• / Found {len(negative_volume)} records with negative volume")
            
            # æ£€æŸ¥æ•°æ®é‡ / Check data volume
            if len(df) < 10:
                errors.append(f"æ•°æ®é‡è¿‡å°‘ / Insufficient data: {len(df)} rows (minimum 10 required)")
            
            return len(errors) == 0, errors
            
        except Exception as e:
            errors.append(f"æ•°æ®éªŒè¯å¼‚å¸¸ / Data validation exception: {str(e)}")
            return False, errors
    
    @staticmethod
    def clean_data(df: pd.DataFrame, remove_outliers: bool = True, fill_missing: bool = True) -> pd.DataFrame:
        """
        æ¸…æ´—æ•°æ® / Clean data
        
        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ® / Raw data
            remove_outliers (bool): æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼ / Whether to remove outliers
            fill_missing (bool): æ˜¯å¦å¡«è¡¥ç¼ºå¤±å€¼ / Whether to fill missing values
            
        Returns:
            pd.DataFrame: æ¸…æ´—åçš„æ•°æ® / Cleaned data
        """
        cleaned_df = df.copy()
        
        try:
            # å¡«è¡¥ç¼ºå¤±å€¼ / Fill missing values
            if fill_missing:
                numeric_columns = ['open', 'high', 'low', 'close', 'volume']
                for col in numeric_columns:
                    if col in cleaned_df.columns:
                        # ä½¿ç”¨å‰å‘å¡«å……å’Œåå‘å¡«å…… / Use forward fill and backward fill
                        cleaned_df[col] = cleaned_df[col].fillna(method='ffill').fillna(method='bfill')
            
            # ç§»é™¤å¼‚å¸¸å€¼ / Remove outliers
            if remove_outliers:
                price_columns = ['open', 'high', 'low', 'close']
                for col in price_columns:
                    if col in cleaned_df.columns:
                        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ / Use IQR method to detect outliers
                        Q1 = cleaned_df[col].quantile(0.25)
                        Q3 = cleaned_df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        
                        # å°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºè¾¹ç•Œå€¼ / Replace outliers with boundary values
                        cleaned_df[col] = cleaned_df[col].clip(lower=lower_bound, upper=upper_bound)
            
            # ç¡®ä¿ä»·æ ¼é€»è¾‘æ­£ç¡® / Ensure price logic is correct
            if all(col in cleaned_df.columns for col in ['open', 'high', 'low', 'close']):
                # ä¿®æ­£æœ€é«˜ä»·å’Œæœ€ä½ä»· / Correct high and low prices
                cleaned_df['high'] = cleaned_df[['open', 'high', 'low', 'close']].max(axis=1)
                cleaned_df['low'] = cleaned_df[['open', 'high', 'low', 'close']].min(axis=1)
            
            return cleaned_df
            
        except Exception as e:
            logging.error(f"æ•°æ®æ¸…æ´—å¤±è´¥ / Data cleaning failed: {str(e)}")
            return df

class ConfigManager:
    """
    é…ç½®ç®¡ç†å™¨ç±» / Configuration Manager Class
    
    æä¾›é…ç½®æ–‡ä»¶çš„åŠ è½½ã€ä¿å­˜å’Œç®¡ç†åŠŸèƒ½ã€‚
    Provides configuration file loading, saving, and management functions.
    """
    
    @staticmethod
    def load_yaml_config(file_path: str) -> Dict[str, Any]:
        """
        åŠ è½½YAMLé…ç½®æ–‡ä»¶ / Load YAML configuration file
        
        Args:
            file_path (str): é…ç½®æ–‡ä»¶è·¯å¾„ / Configuration file path
            
        Returns:
            Dict[str, Any]: é…ç½®å­—å…¸ / Configuration dictionary
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                config = yaml.safe_load(file)
            
            logging.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ / Configuration loaded successfully: {file_path}")
            return config
            
        except FileNotFoundError:
            logging.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ / Configuration file not found: {file_path}")
            return {}
        except yaml.YAMLError as e:
            logging.error(f"âŒ YAMLè§£æé”™è¯¯ / YAML parsing error: {str(e)}")
            return {}
        except Exception as e:
            logging.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ / Failed to load configuration: {str(e)}")
            return {}
    
    @staticmethod
    def save_yaml_config(config: Dict[str, Any], file_path: str) -> bool:
        """
        ä¿å­˜YAMLé…ç½®æ–‡ä»¶ / Save YAML configuration file
        
        Args:
            config (Dict[str, Any]): é…ç½®å­—å…¸ / Configuration dictionary
            file_path (str): é…ç½®æ–‡ä»¶è·¯å¾„ / Configuration file path
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ / Whether saved successfully
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨ / Ensure directory exists
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as file:
                yaml.dump(config, file, default_flow_style=False, allow_unicode=True, indent=2)
            
            logging.info(f"âœ… é…ç½®æ–‡ä»¶ä¿å­˜æˆåŠŸ / Configuration saved successfully: {file_path}")
            return True
            
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥ / Failed to save configuration: {str(e)}")
            return False
    
    @staticmethod
    def merge_configs(base_config: Dict[str, Any], override_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶é…ç½®å­—å…¸ / Merge configuration dictionaries
        
        Args:
            base_config (Dict[str, Any]): åŸºç¡€é…ç½® / Base configuration
            override_config (Dict[str, Any]): è¦†ç›–é…ç½® / Override configuration
            
        Returns:
            Dict[str, Any]: åˆå¹¶åçš„é…ç½® / Merged configuration
        """
        merged = base_config.copy()
        
        for key, value in override_config.items():
            if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
                merged[key] = ConfigManager.merge_configs(merged[key], value)
            else:
                merged[key] = value
        
        return merged

class FileManager:
    """
    æ–‡ä»¶ç®¡ç†å™¨ç±» / File Manager Class
    
    æä¾›æ–‡ä»¶å’Œç›®å½•æ“ä½œçš„è¾…åŠ©åŠŸèƒ½ã€‚
    Provides helper functions for file and directory operations.
    """
    
    @staticmethod
    def ensure_directory(directory_path: str) -> bool:
        """
        ç¡®ä¿ç›®å½•å­˜åœ¨ / Ensure directory exists
        
        Args:
            directory_path (str): ç›®å½•è·¯å¾„ / Directory path
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ / Whether successful
        """
        try:
            os.makedirs(directory_path, exist_ok=True)
            return True
        except Exception as e:
            logging.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ / Failed to create directory: {str(e)}")
            return False
    
    @staticmethod
    def load_csv_data(file_path: str, parse_dates: Optional[List[str]] = None) -> pd.DataFrame:
        """
        åŠ è½½CSVæ•°æ®æ–‡ä»¶ / Load CSV data file
        
        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„ / File path
            parse_dates (List[str], optional): éœ€è¦è§£æä¸ºæ—¥æœŸçš„åˆ— / Columns to parse as dates
            
        Returns:
            pd.DataFrame: æ•°æ®æ¡† / DataFrame
        """
        try:
            df = pd.read_csv(file_path, parse_dates=parse_dates)
            logging.info(f"âœ… CSVæ•°æ®åŠ è½½æˆåŠŸ / CSV data loaded successfully: {file_path}, å½¢çŠ¶: {df.shape}")
            return df
        except Exception as e:
            logging.error(f"âŒ åŠ è½½CSVæ•°æ®å¤±è´¥ / Failed to load CSV data: {str(e)}")
            return pd.DataFrame()
    
    @staticmethod
    def save_csv_data(df: pd.DataFrame, file_path: str, index: bool = False) -> bool:
        """
        ä¿å­˜CSVæ•°æ®æ–‡ä»¶ / Save CSV data file
        
        Args:
            df (pd.DataFrame): æ•°æ®æ¡† / DataFrame
            file_path (str): æ–‡ä»¶è·¯å¾„ / File path
            index (bool): æ˜¯å¦ä¿å­˜ç´¢å¼• / Whether to save index
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ / Whether saved successfully
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨ / Ensure directory exists
            FileManager.ensure_directory(os.path.dirname(file_path))
            
            df.to_csv(file_path, index=index, encoding='utf-8')
            logging.info(f"âœ… CSVæ•°æ®ä¿å­˜æˆåŠŸ / CSV data saved successfully: {file_path}")
            return True
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜CSVæ•°æ®å¤±è´¥ / Failed to save CSV data: {str(e)}")
            return False

class MathUtils:
    """
    æ•°å­¦å·¥å…·ç±» / Math Utilities Class
    
    æä¾›æ•°å­¦å’Œç»Ÿè®¡è®¡ç®—åŠŸèƒ½ã€‚
    Provides mathematical and statistical calculation functions.
    """
    
    @staticmethod
    def calculate_returns(prices: pd.Series) -> pd.Series:
        """
        è®¡ç®—æ”¶ç›Šç‡ / Calculate returns
        
        Args:
            prices (pd.Series): ä»·æ ¼åºåˆ— / Price series
            
        Returns:
            pd.Series: æ”¶ç›Šç‡åºåˆ— / Returns series
        """
        return prices.pct_change().dropna()
    
    @staticmethod
    def calculate_volatility(returns: pd.Series, annualize: bool = True) -> float:
        """
        è®¡ç®—æ³¢åŠ¨ç‡ / Calculate volatility
        
        Args:
            returns (pd.Series): æ”¶ç›Šç‡åºåˆ— / Returns series
            annualize (bool): æ˜¯å¦å¹´åŒ– / Whether to annualize
            
        Returns:
            float: æ³¢åŠ¨ç‡ / Volatility
        """
        vol = returns.std()
        if annualize:
            vol *= np.sqrt(252)  # å‡è®¾ä¸€å¹´252ä¸ªäº¤æ˜“æ—¥ / Assume 252 trading days per year
        return vol
    
    @staticmethod
    def calculate_sharpe_ratio(returns: pd.Series, risk_free_rate: float = 0.02) -> float:
        """
        è®¡ç®—å¤æ™®æ¯”ç‡ / Calculate Sharpe ratio
        
        Args:
            returns (pd.Series): æ”¶ç›Šç‡åºåˆ— / Returns series
            risk_free_rate (float): æ— é£é™©åˆ©ç‡ / Risk-free rate
            
        Returns:
            float: å¤æ™®æ¯”ç‡ / Sharpe ratio
        """
        if returns.std() == 0:
            return 0.0
        
        excess_returns = returns - risk_free_rate / 252  # æ—¥æ”¶ç›Šç‡ / Daily returns
        return excess_returns.mean() / returns.std() * np.sqrt(252)
    
    @staticmethod
    def calculate_max_drawdown(prices: pd.Series) -> float:
        """
        è®¡ç®—æœ€å¤§å›æ’¤ / Calculate maximum drawdown
        
        Args:
            prices (pd.Series): ä»·æ ¼åºåˆ— / Price series
            
        Returns:
            float: æœ€å¤§å›æ’¤æ¯”ä¾‹ / Maximum drawdown ratio
        """
        peak = prices.expanding().max()
        drawdown = (prices - peak) / peak
        return drawdown.min()
    
    @staticmethod
    def calculate_calmar_ratio(returns: pd.Series) -> float:
        """
        è®¡ç®—å¡å°”ç›æ¯”ç‡ / Calculate Calmar ratio
        
        Args:
            returns (pd.Series): æ”¶ç›Šç‡åºåˆ— / Returns series
            
        Returns:
            float: å¡å°”ç›æ¯”ç‡ / Calmar ratio
        """
        if len(returns) < 2:
            return 0.0
        
        cumulative_returns = (1 + returns).cumprod()
        annual_return = cumulative_returns.iloc[-1] ** (252 / len(returns)) - 1
        max_dd = MathUtils.calculate_max_drawdown(cumulative_returns)
        
        if max_dd == 0:
            return float('inf')
        
        return annual_return / abs(max_dd)

class LoggingUtils:
    """
    æ—¥å¿—å·¥å…·ç±» / Logging Utilities Class
    
    æä¾›æ—¥å¿—è®°å½•çš„è¾…åŠ©åŠŸèƒ½ã€‚
    Provides helper functions for logging.
    """
    
    @staticmethod
    def setup_logger(name: str, log_file: Optional[str] = None, level: int = logging.INFO) -> logging.Logger:
        """
        è®¾ç½®æ—¥å¿—è®°å½•å™¨ / Setup logger
        
        Args:
            name (str): è®°å½•å™¨åç§° / Logger name
            log_file (str, optional): æ—¥å¿—æ–‡ä»¶è·¯å¾„ / Log file path
            level (int): æ—¥å¿—çº§åˆ« / Log level
            
        Returns:
            logging.Logger: é…ç½®å¥½çš„è®°å½•å™¨ / Configured logger
        """
        logger = logging.getLogger(name)
        logger.setLevel(level)
        
        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨ / Avoid duplicate handlers
        if logger.handlers:
            return logger
        
        # åˆ›å»ºæ ¼å¼å™¨ / Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # æ§åˆ¶å°å¤„ç†å™¨ / Console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨ / File handler
        if log_file:
            FileManager.ensure_directory(os.path.dirname(log_file))
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    @staticmethod
    def log_performance_metrics(logger: logging.Logger, metrics: Dict[str, Any]):
        """
        è®°å½•æ€§èƒ½æŒ‡æ ‡ / Log performance metrics
        
        Args:
            logger (logging.Logger): æ—¥å¿—è®°å½•å™¨ / Logger
            metrics (Dict[str, Any]): æ€§èƒ½æŒ‡æ ‡å­—å…¸ / Performance metrics dictionary
        """
        logger.info("ğŸ“Š ===== æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š / Performance Metrics Report =====")
        
        for category, values in metrics.items():
            if isinstance(values, dict):
                logger.info(f"ğŸ“‹ {category}:")
                for key, value in values.items():
                    if isinstance(value, float):
                        logger.info(f"  {key}: {value:.4f}")
                    else:
                        logger.info(f"  {key}: {value}")
            else:
                if isinstance(values, float):
                    logger.info(f"ğŸ“Š {category}: {values:.4f}")
                else:
                    logger.info(f"ğŸ“Š {category}: {values}")
        
        logger.info("ğŸ“Š ===== æŠ¥å‘Šç»“æŸ / End of Report =====")

# å¯¼å‡ºæ‰€æœ‰å·¥å…·ç±» / Export all utility classes
__all__ = [
    'TechnicalIndicators',
    'DataValidator', 
    'ConfigManager',
    'FileManager',
    'MathUtils',
    'LoggingUtils'
] 